{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b71z5TdZpbuz"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "%%capture\n",
        "!rm seq2seq_trainer.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
        "\n",
        "!pip install git-python==1.0.3\n",
        "!pip install sacrebleu==1.4.12\n",
        "!pip install rouge_score\n",
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import TrainingArguments\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datasets\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import EncoderDecoderModel\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import TrainingArguments\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n"
      ],
      "metadata": {
        "id": "A6VGSm1o5XxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token\n",
        "tokenizer.sep_token = tokenizer.sep_token\n",
        "\n",
        "data = datasets.load_dataset('scientific_papers','arxiv')\n",
        "\n",
        "train_data_full = data['train']\n",
        "val_data_full = data['validation']\n",
        "test_data_full = data['test']\n",
        "\n",
        "train_data=train_data_full\n",
        "val_data = val_data_full\n",
        "test_data = test_data_full\n",
        "\n",
        "batch_size = 64\n"
      ],
      "metadata": {
        "id": "NSE1AAxX5VTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "  # tokenize the inputs and labels\n",
        "  inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = tokenizer(batch[\"abstract\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "  # inputs = my_custom_tokenizer(batch[\"article\"], 512) # Need to resolve the technical tensor dimension based errors from this function and use this method to fine-tune\n",
        "  # outputs = my_custom_tokenizer(batch[\"abstract\"], 128)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
        "  batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
        "  batch[\"decoder_input_ids\"] = outputs[\"input_ids\"]\n",
        "  batch[\"decoder_attention_mask\"] = outputs[\"attention_mask\"]\n",
        "  batch[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "  return batch\n"
      ],
      "metadata": {
        "id": "eUnFtSC647NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize document into sentences\n",
        "def my_custom_tokenizer(batch,max_length):\n",
        "\n",
        "  pad_length = max_length\n",
        "  ret = {}\n",
        "  ret['input_ids']=[]\n",
        "  ret['attention_mask']=[]\n",
        "  for b in batch:\n",
        "    for para in b :\n",
        "      doc = para\n",
        "      sentences = nltk.sent_tokenize(doc)\n",
        "\n",
        "      # Initialize lists for tokens, input IDs, and attention masks\n",
        "      tokens = []\n",
        "      input_ids = []\n",
        "      attention_masks = []\n",
        "\n",
        "      # Add CLS token at the beginning of the article\n",
        "      tokens.append(\"[CLS]\")\n",
        "\n",
        "      # Loop through each sentence and tokenize it\n",
        "      for sentence in sentences:\n",
        "          # Tokenize the sentence and append the tokens\n",
        "          sentence_tokens = tokenizer.tokenize(sentence)\n",
        "          tokens += sentence_tokens\n",
        "\n",
        "          # Create input IDs and attention masks for the tokens\n",
        "          sentence_input_ids = tokenizer.convert_tokens_to_ids(sentence_tokens)\n",
        "          sentence_attention_masks = [1] * len(sentence_input_ids)\n",
        "\n",
        "          # Append the input IDs and attention masks for the sentence\n",
        "          input_ids += sentence_input_ids\n",
        "          attention_masks += sentence_attention_masks\n",
        "\n",
        "          # Add SEP token after the sentence\n",
        "          tokens.append(\"[SEP]\")\n",
        "          input_ids.append(tokenizer.sep_token_id)\n",
        "          attention_masks.append(1)\n",
        "\n",
        "      # Truncate the input if it exceeds the maximum length\n",
        "      if len(input_ids) > max_length:\n",
        "          input_ids = input_ids[:max_length]\n",
        "          attention_masks = attention_masks[:max_length]\n",
        "\n",
        "      # Pad the input if it is shorter than the fixed length\n",
        "      if len(input_ids) < pad_length:\n",
        "          padding_length = pad_length - len(input_ids)\n",
        "          input_ids = input_ids + ([tokenizer.pad_token_id] * padding_length)\n",
        "          attention_masks = attention_masks + ([0] * padding_length)\n",
        "\n",
        "      # Convert tokens to input IDs and attention masks\n",
        "      input_ids = [tokenizer.cls_token_id] + input_ids\n",
        "      attention_masks = [1] + attention_masks\n",
        "\n",
        "      ret['input_ids'].append(input_ids)\n",
        "      ret['attention_mask'].append(attention_masks)\n",
        "\n",
        "  return ret"
      ],
      "metadata": {
        "id": "O7S4IbvFGulD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=train_data_full\n",
        "val_data = val_data_full\n",
        "\n",
        "train_data = train_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"]\n",
        ")\n",
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "\n",
        "val_data= val_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"]\n",
        ")\n",
        "val_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "yXMS79qSHMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert = EncoderDecoderModel.from_pretrained('patrickvonplaten/bert2bert_cnn_daily_mail')\n",
        "\n",
        "count = 0\n",
        "for param in bert2bert.parameters():\n",
        "  if count == 250:\n",
        "    break\n",
        "  param.requires_grad = False\n",
        "  count = count +1\n"
      ],
      "metadata": {
        "id": "eiKm0SmT45he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Seq2SeqTrainingArguments(TrainingArguments):\n",
        "    label_smoothing: Optional[float] = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
        "    )\n",
        "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
        "    encoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    decoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
        "    attention_dropout: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    lr_scheduler: Optional[str] = field(\n",
        "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
        "    )\n",
        "    generation_config : Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Goes into model.config\"}\n",
        "    )\n",
        "    evaluate_during_training : bool = field(\n",
        "        default=True, metadata={\"help\": \"evaluate during training\"}\n",
        "    )\n",
        "\n",
        "rouge = datasets.load_metric(\"rouge\")\n"
      ],
      "metadata": {
        "id": "O3gJaREA4zZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "IlvtYdqi4xKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    predict_with_generate=True,\n",
        "    evaluate_during_training=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    logging_steps=1000,  \n",
        "    save_steps=500,  \n",
        "    eval_steps=8000,  \n",
        "    warmup_steps=2000,  \n",
        "    # max_steps=16, \n",
        "    overwrite_output_dir=True,\n",
        "    save_total_limit=3,\n",
        "    fp16=True\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=bert2bert,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")"
      ],
      "metadata": {
        "id": "Jx1N8QUe4sDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "rs5YIRTW4qt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = bert2bert\n",
        "model.to(\"cuda\")\n",
        "\n",
        "test_data = data['test']\n",
        "\n",
        "batch_size = 5"
      ],
      "metadata": {
        "id": "M-pO-xzR4nz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(batch):\n",
        "\n",
        "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    batch[\"pred\"] = output_str\n",
        "\n",
        "    return batch\n",
        "\n",
        "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
        "\n",
        "pred_str = results[\"pred\"]\n",
        "label_str = results[\"abstract\"]"
      ],
      "metadata": {
        "id": "nfAUl49t4kB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STzPQBCG7OVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}